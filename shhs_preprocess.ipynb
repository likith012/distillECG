{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, datetime, operator, logging, sys\n",
    "from collections import namedtuple\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import math\n",
    "import ntpath\n",
    "\n",
    "import shutil\n",
    "import urllib\n",
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "EVENT_CHANNEL = 'EDF Annotations'\n",
    "\n",
    "class EDFEndOfData(Exception): pass\n",
    "\n",
    "def tal(tal_str):\n",
    "    '''Return a list with (onset, duration, annotation) tuples for an EDF+ TAL\n",
    "  stream.\n",
    "  '''\n",
    "    exp = '(?P<onset>[+\\-]\\d+(?:\\.\\d*)?)' + '(?:\\x15(?P<duration>\\d+(?:\\.\\d*)?))?' + '(\\x14(?P<annotation>[^\\x00]*))?' + '(?:\\x14\\x00)'\n",
    "\n",
    "    def annotation_to_list(annotation):\n",
    "        return str(annotation.encode('utf-8')).split('\\x14') if annotation else []\n",
    "\n",
    "    def parse(dic):\n",
    "        return (\n",
    "      float(dic['onset']),\n",
    "      float(dic['duration']) if dic['duration'] else 0.,\n",
    "      annotation_to_list(dic['annotation']))\n",
    "\n",
    "    return [parse(m.groupdict()) for m in re.finditer(exp, tal_str)]\n",
    "\n",
    "\n",
    "def edf_header(f):\n",
    "    h = {}\n",
    "    assert f.tell() == 0  # check file position\n",
    "    assert f.read(8) == '0       '\n",
    "\n",
    "    # recording info)\n",
    "    h['local_subject_id'] = f.read(80).strip()\n",
    "    h['local_recording_id'] = f.read(80).strip()\n",
    "\n",
    "    # parse timestamp\n",
    "    (day, month, year) = [int(x) for x in re.findall('(\\d+)', f.read(8))]\n",
    "    (hour, minute, sec)= [int(x) for x in re.findall('(\\d+)', f.read(8))]\n",
    "    h['date_time'] = str(datetime.datetime(year + 2000, month, day,\n",
    "    hour, minute, sec))\n",
    "\n",
    "    # misc\n",
    "    header_nbytes = int(f.read(8))\n",
    "    subtype = f.read(44)[:5]\n",
    "    h['EDF+'] = subtype in ['EDF+C', 'EDF+D']\n",
    "    h['contiguous'] = subtype != 'EDF+D'\n",
    "    h['n_records'] = int(f.read(8))\n",
    "    h['record_length'] = float(f.read(8))  # in seconds\n",
    "    nchannels = h['n_channels'] = int(f.read(4))\n",
    "\n",
    "    # read channel info\n",
    "    channels = range(h['n_channels'])\n",
    "    h['label'] = [f.read(16).strip() for n in channels]\n",
    "    h['transducer_type'] = [f.read(80).strip() for n in channels]\n",
    "    h['units'] = [f.read(8).strip() for n in channels]\n",
    "    h['physical_min'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "    h['physical_max'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "    h['digital_min'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "    h['digital_max'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "    h['prefiltering'] = [f.read(80).strip() for n in channels]\n",
    "    h['n_samples_per_record'] = [int(f.read(8)) for n in channels]\n",
    "    f.read(32 * nchannels)  # reserved\n",
    "\n",
    "    #assert f.tell() == header_nbytes\n",
    "    return h\n",
    "\n",
    "\n",
    "class BaseEDFReader:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "\n",
    "\n",
    "    def read_header(self):\n",
    "        self.header = h = edf_header(self.file)\n",
    "\n",
    "        # calculate ranges for rescaling\n",
    "        self.dig_min = h['digital_min']\n",
    "        self.phys_min = h['physical_min']\n",
    "        phys_range = h['physical_max'] - h['physical_min']\n",
    "        dig_range = h['digital_max'] - h['digital_min']\n",
    "        assert np.all(phys_range > 0)\n",
    "        assert np.all(dig_range > 0)\n",
    "        self.gain = phys_range / dig_range\n",
    "\n",
    "\n",
    "    def read_raw_record(self):\n",
    "        '''Read a record with data_2013 and return a list containing arrays with raw\n",
    "        bytes.\n",
    "        '''\n",
    "        result = []\n",
    "        for nsamp in self.header['n_samples_per_record']:\n",
    "            samples = self.file.read(nsamp * 2)\n",
    "            if len(samples) != nsamp * 2:\n",
    "                raise EDFEndOfData\n",
    "            result.append(samples)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def convert_record(self, raw_record):\n",
    "        '''Convert a raw record to a (time, signals, events) tuple based on\n",
    "        information in the header.\n",
    "        '''\n",
    "        h = self.header\n",
    "        dig_min, phys_min, gain = self.dig_min, self.phys_min, self.gain\n",
    "        time = float('nan')\n",
    "        signals = []\n",
    "        events = []\n",
    "        for (i, samples) in enumerate(raw_record):\n",
    "            if h['label'][i] == EVENT_CHANNEL:\n",
    "                ann = tal(samples)\n",
    "                time = ann[0][0]\n",
    "                events.extend(ann[1:])\n",
    "                # print(i, samples)\n",
    "                # exit()\n",
    "            else:\n",
    "                # 2-byte little-endian integers\n",
    "                dig = np.fromstring(samples, '<i2').astype(np.float32)\n",
    "                phys = (dig - dig_min[i]) * gain[i] + phys_min[i]\n",
    "                signals.append(phys)\n",
    "\n",
    "        return time, signals, events\n",
    "\n",
    "\n",
    "    def read_record(self):\n",
    "        return self.convert_record(self.read_raw_record())\n",
    "\n",
    "\n",
    "    def records(self):\n",
    "        '''\n",
    "        Record generator.\n",
    "        '''\n",
    "        try:\n",
    "            while True:\n",
    "                yield self.read_record()\n",
    "        except EDFEndOfData:\n",
    "            pass\n",
    "\n",
    "\n",
    "def load_edf(edffile):\n",
    "    '''Load an EDF+ file.\n",
    "  Very basic reader for EDF and EDF+ files. While BaseEDFReader does support\n",
    "  exotic features like non-homogeneous sample rates and loading only parts of\n",
    "  the stream, load_edf expects a single fixed sample rate for all channels and\n",
    "  tries to load the whole file.\n",
    "  Parameters\n",
    "  ----------\n",
    "  edffile : file-like object or string\n",
    "  Returns\n",
    "  -------\n",
    "  Named tuple with the fields:\n",
    "    X : NumPy array with shape p by n.\n",
    "      Raw recording of n samples in p dimensions.\n",
    "    sample_rate : float\n",
    "      The sample rate of the recording. Note that mixed sample-rates are not\n",
    "      supported.\n",
    "    sens_lab : list of length p with strings\n",
    "      The labels of the sensors used to record X.\n",
    "    time : NumPy array with length n\n",
    "      The time offset in the recording for each sample.\n",
    "    annotations : a list with tuples\n",
    "      EDF+ annotations are stored in (start, duration, description) tuples.\n",
    "      start : float\n",
    "        Indicates the start of the event in seconds.\n",
    "      duration : float\n",
    "        Indicates the duration of the event in seconds.\n",
    "      description : list with strings\n",
    "        Contains (multiple?) descriptions of the annotation event.\n",
    "  '''\n",
    "    if isinstance(edffile, basestring):\n",
    "        with open(edffile, 'rb') as f:\n",
    "            return load_edf(f)  # convert filename to file\n",
    "\n",
    "    reader = BaseEDFReader(edffile)\n",
    "    reader.read_header()\n",
    "\n",
    "    h = reader.header\n",
    "    log.debug('EDF header: %s' % h)\n",
    "\n",
    "      # get sample rate info\n",
    "    nsamp = np.unique(\n",
    "        [n for (l, n) in zip(h['label'], h['n_samples_per_record'])\n",
    "        if l != EVENT_CHANNEL])\n",
    "    assert nsamp.size == 1, 'Multiple sample rates not supported!'\n",
    "    sample_rate = float(nsamp[0]) / h['record_length']\n",
    "\n",
    "    rectime, X, annotations = zip(*reader.records())\n",
    "    X = np.hstack(X)\n",
    "    annotations = reduce(operator.add, annotations)\n",
    "    chan_lab = [lab for lab in reader.header['label'] if lab != EVENT_CHANNEL]\n",
    "\n",
    "      # create timestamps\n",
    "    if reader.header['contiguous']:\n",
    "        time = np.arange(X.shape[1]) / sample_rate\n",
    "    else:\n",
    "        reclen = reader.header['record_length']\n",
    "        within_rec_time = np.linspace(0, reclen, nsamp, endpoint=False)\n",
    "        time = np.hstack([t + within_rec_time for t in rectime])\n",
    "\n",
    "    tup = namedtuple('EDF', 'X sample_rate chan_lab time annotations')\n",
    "    return tup(X, sample_rate, chan_lab, time, annotations)\n",
    "\n",
    "\n",
    "EPOCH_SEC_SIZE = 30\n",
    "\n",
    "# data on GNODE 25 DATE: 06-12-21 (ALL 329 files of SHHS1)\n",
    "\n",
    "\n",
    "data_dir = '/scratch/SLEEP_data/shhs/polysomnography/edfs/shhs1'\n",
    "ann_dir = '/scratch/SLEEP_data/shhs/polysomnography/annotations-events-profusion/shhs1'\n",
    "output_dir = '/scratch/SLEEP_data/shhs/output'\n",
    "select_ch = 'EEG C4-A1'  #EEG (sec)\tC3\tA2  #EEG\tC4\tA1\n",
    "\n",
    "csv_path = '/scratch/SLEEP_data/selected_shhs1_files.txt'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "#ids = pd.read_csv(\"selected_shhs1_files.txt\", header=None, names='a')\n",
    "ids = pd.read_csv(csv_path, header=None)\n",
    "ids = ids[0].values.tolist()\n",
    "\n",
    "edf_fnames = [os.path.join(data_dir, i + \".edf\") for i in ids]\n",
    "ann_fnames = [os.path.join(ann_dir,  i + \"-profusion.xml\") for i in ids]\n",
    "\n",
    "edf_fnames.sort()\n",
    "ann_fnames.sort()\n",
    "\n",
    "edf_fnames = np.asarray(edf_fnames)\n",
    "ann_fnames = np.asarray(ann_fnames)\n",
    "\n",
    "#yahase \n",
    "for file_id in range(len(edf_fnames)):\n",
    "    if os.path.exists(os.path.join(output_dir, edf_fnames[file_id].split('/')[-1])[:-4]+\".npz\"):\n",
    "        continue\n",
    "    print(edf_fnames[file_id])\n",
    "    select_ch = 'EEG C4-A1'\n",
    "    raw = read_raw_edf(edf_fnames[file_id], preload=True, stim_channel=None, verbose=None)\n",
    "    sampling_rate = raw.info['sfreq']\n",
    "    ch_type = select_ch.split(\" \")[0]    # selecting EEG out of 'EEG C4-A1'\n",
    "    select_ch = sorted([s for s in raw.info[\"ch_names\"] if ch_type in s]) # this has 2 vals [EEG,EEG(sec)] and selecting 0th index\n",
    "    print(select_ch)\n",
    "    raw_ch_df = raw.to_data_frame(scalings=sampling_rate)[select_ch]\n",
    "    print(raw_ch_df.shape)\n",
    "    #raw_ch_df = raw_ch_df.to_frame()\n",
    "    raw_ch_df.set_index(np.arange(len(raw_ch_df)))\n",
    "  \n",
    "    labels = []\n",
    "    # Read annotation and its header\n",
    "    t = ET.parse(ann_fnames[file_id])\n",
    "    r = t.getroot()\n",
    "    faulty_File = 0\n",
    "    for i in range(len(r[4])):\n",
    "        lbl = int(r[4][i].text)\n",
    "        if lbl == 4:  # make stages N3, N4 same as N3\n",
    "            labels.append(3)\n",
    "        elif lbl == 5:  # Assign label 4 for REM stage\n",
    "            labels.append(4)\n",
    "        else:\n",
    "            labels.append(lbl)\n",
    "        if lbl > 5:  # some files may contain labels > 5 BUT not the selected ones.\n",
    "            faulty_File = 1\n",
    "\n",
    "    if faulty_File == 1:\n",
    "        print( \"============================== Faulty file ==================\")\n",
    "        continue\n",
    "\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # Remove movement and unknown stages if any\n",
    "    raw_ch = raw_ch_df.values\n",
    "    print(raw_ch.shape)\n",
    "\n",
    "    # Verify that we can split into 30-s epochs\n",
    "    if len(raw_ch) % (EPOCH_SEC_SIZE * sampling_rate) != 0:\n",
    "        raise Exception(\"Something wrong\")\n",
    "    n_epochs = len(raw_ch) / (EPOCH_SEC_SIZE * sampling_rate)\n",
    "\n",
    "    # Get epochs and their corresponding labels\n",
    "    x = np.asarray(np.split(raw_ch, n_epochs)).astype(np.float32)\n",
    "    y = labels.astype(np.int32)\n",
    "\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    # Select on sleep periods\n",
    "    w_edge_mins = 30\n",
    "    nw_idx = np.where(y != 0)[0]\n",
    "    start_idx = nw_idx[0] - (w_edge_mins * 2)\n",
    "    end_idx = nw_idx[-1] + (w_edge_mins * 2)\n",
    "    if start_idx < 0: start_idx = 0\n",
    "    if end_idx >= len(y): end_idx = len(y) - 1\n",
    "    select_idx = np.arange(start_idx, end_idx + 1)\n",
    "    print(\"Data before selection: {}, {}\".format(x.shape, y.shape))\n",
    "    x = x[select_idx]\n",
    "    y = y[select_idx]\n",
    "    print(\"Data after selection: {}, {}\".format(x.shape, y.shape))\n",
    "\n",
    "    # Saving as numpy files\n",
    "    filename = os.path.basename(edf_fnames[file_id]).replace(\".edf\",  \".npz\")\n",
    "    save_dict = {\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"fs\": sampling_rate\n",
    "    }\n",
    "    np.savez(os.path.join(output_dir, filename), **save_dict)\n",
    "    print(\" ---------- Done this file ---------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'SleepStage' at 0x000002442C6B87C0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SELECTED_SUBJECTS_PATH = './preprocess/shhs/selected_shhs1.txt'\n",
    "\n",
    "\n",
    "window_size = 30\n",
    "sfreq = 100\n",
    "window_size_samples = window_size*sfreq\n",
    "subject_ids = pd.read_csv(SELECTED_SUBJECTS_PATH, header=None)\n",
    "subject_ids = subject_ids[0].values.tolist()\n",
    "SHHS_PATH = '/scratch/shhs/edfs/shhs1'\n",
    "SHHS_EVENTS_PATH = '/scratch/shhs/annotations-events-profusion'\n",
    "SELECTED_SUBJECTS_PATH = './preprocess/shhs/selected_shhs1.txt'\n",
    "SHHS_SAVE_PATH = os.path.join(os.path.split(os.path.split(SHHS_PATH)[0])[0], 'subjects_data')\n",
    "\n",
    "subject_ids = pd.read_csv(SELECTED_SUBJECTS_PATH, header=None)\n",
    "subject_ids\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "['SaO2',\n",
    " 'H.R.',\n",
    " 'EEG(sec)',\n",
    " 'ECG',\n",
    " 'EMG',\n",
    " 'EOG(L)',\n",
    " 'EOG(R)',\n",
    " 'EEG',\n",
    " 'SOUND',\n",
    " 'AIRFLOW',\n",
    " 'THOR RES',\n",
    " 'ABDO RES',\n",
    " 'POSITION',\n",
    " 'LIGHT',\n",
    " 'NEW AIR',\n",
    " 'OX stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mne\n",
    "import xml.etree.ElementTree as ET\n",
    "from braindecode.preprocessing.preprocess import preprocess, Preprocessor, zscore\n",
    "from braindecode.preprocessing.windowers import create_windows_from_events\n",
    "from braindecode.datasets import BaseConcatDataset, BaseDataset\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "SHHS_PATH = '/scratch/shhs/edfs/shhs1'\n",
    "SHHS_EVENTS_PATH = '/scratch/shhs/annotations-events-profusion'\n",
    "SELECTED_SUBJECTS_PATH = './preprocess/shhs/selected_shhs1.txt'\n",
    "SHHS_SAVE_PATH = os.path.join(os.path.split(os.path.split(SHHS_PATH)[0])[0], 'subjects_data')\n",
    "\n",
    "if not os.path.exists(SHHS_SAVE_PATH):\n",
    "    os.makedirs(SHHS_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "window_size = 30\n",
    "sfreq = 100\n",
    "window_size_samples = window_size*sfreq\n",
    "subject_ids = pd.read_csv(SELECTED_SUBJECTS_PATH, header=None)\n",
    "subject_ids = subject_ids[0].values.tolist()\n",
    "raw_paths = [os.path.join(SHHS_PATH, f'{f}.edf') for f in subject_ids]\n",
    "ann_paths = [os.path.join(SHHS_PATH, f'{f}-profusion.xml') for f in subject_ids]\n",
    "\n",
    "label_mapping = {  \n",
    "    \"Sleep stage W\": 0,\n",
    "    \"Sleep stage N1\": 1,\n",
    "    \"Sleep stage N2\": 2,\n",
    "    \"Sleep stage N3\": 3,\n",
    "    \"Sleep stage R\": 4,\n",
    "}\n",
    "channel_mapping = {\n",
    "    'eeg': ['EEG', 'EEG(sec)'],\n",
    "    'ecg': ['ECG'],\n",
    "    'eog': ['EOG(L)', 'EOG(R)'],\n",
    "    'emg': ['EMG'],\n",
    "    'emog': ['EOG(L)', 'EOG(R)', 'EMG'],\n",
    "    'sound': ['SOUND'],\n",
    "}\n",
    "\n",
    "class SHHSSleepStaging(BaseDataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        raw_path=None,\n",
    "        ann_path=None,\n",
    "        channels=None\n",
    "        preload=False,\n",
    "        crop_wake_mins=30,\n",
    "        crop=None,\n",
    "    ):\n",
    "        if (raw_path is None) or (ann_path is None):\n",
    "            raise Exception(\"Please provide paths for raw and annotations file!\")\n",
    "            \n",
    "        self._fetch_data(raw_path, ann_path)\n",
    "\n",
    "        raw, desc = self._load_raw(\n",
    "            raw_path,\n",
    "            ann_path,\n",
    "            preload=preload,\n",
    "            crop_wake_mins=crop_wake_mins,\n",
    "            crop=crop\n",
    "        )\n",
    "        super().__init__(raw, desc)\n",
    "\n",
    "    @staticmethod    \n",
    "    def read_annotations(ann_fname):\n",
    "        labels = []\n",
    "        t = ET.parse(ann_fname)\n",
    "        r = t.getroot()\n",
    "\n",
    "        for i in range(len(r[4])):\n",
    "            lbl = int(r[4][i].text)\n",
    "            if lbl == 0:\n",
    "                labels.append(\"Sleep stage W\")\n",
    "            elif lbl == 1:\n",
    "                labels.append(\"Sleep stage N1\")\n",
    "            elif lbl == 2:\n",
    "                labels.append(\"Sleep stage N2\")\n",
    "            elif (lbl == 3) or (lbl == 4):\n",
    "                labels.append(\"Sleep stage N3\")\n",
    "            elif lbl == 5:\n",
    "                labels.append(\"Sleep stage R\")\n",
    "            else:\n",
    "                print( \"============================== Faulty file =============================\")\n",
    "\n",
    "        labels = np.asarray(labels)\n",
    "        onsets = [window_size*i for i in range(len(labels))]\n",
    "        onsets = np.asarray(onsets)\n",
    "        durations = np.repeat(window_size, len(labels))\n",
    "        annots = mne.Annotations(onsets, durations, labels)\n",
    "        return annots\n",
    "\n",
    "    def _load_raw(\n",
    "        self,\n",
    "        raw_fname,\n",
    "        ann_fname,\n",
    "        preload,\n",
    "        crop_wake_mins,\n",
    "        crop,\n",
    "    ):\n",
    "        raw = mne.io.read_raw_edf(raw_fname, preload=preload)\n",
    "        annots = self.read_annotations(ann_fname)\n",
    "        raw.set_annotations(annots, emit_warning=False)\n",
    "        raw.resample(sfreq, npad=\"auto\")\n",
    "\n",
    "        if crop_wake_mins > 0:\n",
    "            # Find first and last sleep stages\n",
    "            mask = [x[-1] in [\"1\", \"2\", \"3\", \"R\"] for x in annots.description]\n",
    "            sleep_event_inds = np.where(mask)[0]\n",
    "\n",
    "            # Crop raw\n",
    "            tmin = annots[int(sleep_event_inds[0])][\"onset\"] - crop_wake_mins * 60\n",
    "            tmax = annots[int(sleep_event_inds[-1])][\"onset\"] + crop_wake_mins * 60\n",
    "            raw.crop(tmin=max(tmin, raw.times[0]), tmax=min(tmax, raw.times[-1]))\n",
    "\n",
    "        if crop is not None:\n",
    "            raw.crop(*crop)\n",
    "\n",
    "        raw_basename = os.path.basename(raw_fname)\n",
    "        subj_nb = int(raw_basename[2:5])\n",
    "        desc = pd.Series({\"subject_id\": subj_nb,}, name=\"\")\n",
    "        return raw, desc\n",
    "\n",
    "    \n",
    "def __get_epochs(windows_subject):\n",
    "    epochs_data = []\n",
    "    for epoch in windows_subject.windows:\n",
    "        epochs_data.append(epoch)\n",
    "    epochs_data = np.stack(epochs_data, axis=0) # Shape of (num_epochs, num_channels, num_sample_points)\n",
    "    return epochs_data\n",
    "\n",
    "def __get_channels(raw, ann):\n",
    "    channels_data = dict()\n",
    "    for ch in channel_mapping.items():\n",
    "        shhs_subject = SHHSSleepStaging(raw_path=raw, ann_path=ann, channels=channel_mapping[ch], preload=True)\n",
    "        shhs_windows_subject = create_windows_from_events(\n",
    "                                shhs_subject,\n",
    "                                window_size_samples=window_size_samples,\n",
    "                                window_stride_samples=window_size_samples,\n",
    "                                preload=True,\n",
    "                                mapping=label_mapping,\n",
    "                            )\n",
    "        preprocess(shhs_windows_subject, [Preprocessor(zscore)])\n",
    "        channels_data[ch] = __get_epochs(shhs_windows_subject)\n",
    "    channels_data['y'] = shhs_windows_subject.y\n",
    "    channels_data['subject_id'] = shhs_windows_subject.description['subject_id']\n",
    "    channels_data['epoch_length'] = len(shhs_windows_subject)\n",
    "    return channels_data, shhs_windows_subject.description['subject_id']\n",
    "\n",
    "\n",
    "for raw, ann in tqdm(zip(raw_paths, ann_paths), desc=\"SHHS dataset preprocessing ...\"):\n",
    "\n",
    "    channels_data, sub_id = __get_channels(raw, ann)    \n",
    "    subjects_save_path = os.path.join(SHHS_SAVE_PATH, f\"{sub_id:03d}.npz\")\n",
    "    np.savez(subjects_save_path, **channels_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SHHS_PATH = '/scratch/shhs/edfs/shhs1'\n",
    "SHHS_EVENTS_PATH = '/scratch/shhs/annotations-events-profusion'\n",
    "SELECTED_SUBJECTS_PATH = './preprocess/shhs/selected_shhs1.txt'\n",
    "\n",
    "\n",
    "window_size = 30\n",
    "sfreq = 100\n",
    "window_size_samples = window_size*sfreq\n",
    "subject_ids = pd.read_csv(SELECTED_SUBJECTS_PATH, header=None)\n",
    "subject_ids = subject_ids[0].values.tolist()\n",
    "\n",
    "\n",
    "mapping = {  \n",
    "    \"Sleep stage W\": 0,\n",
    "    \"Sleep stage N1\": 1,\n",
    "    \"Sleep stage N2\": 2,\n",
    "    \"Sleep stage N3\": 3,\n",
    "    \"Sleep stage R\": 4,\n",
    "}\n",
    "\n",
    "\n",
    "class SHHSSleepStaging(BaseConcatDataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        shhs_path=None,\n",
    "        subject_ids=None,\n",
    "        preload=False,\n",
    "        crop_wake_mins=0,\n",
    "        crop=None,\n",
    "    ):\n",
    "        if subject_ids is None:\n",
    "            subject_ids = range(1, 155)\n",
    "        if shhs_path is None:\n",
    "            raise Exception(\"Please provide path\")\n",
    "        \n",
    "        self.raw_files, self.edf_files = [], []       \n",
    "        self._fetch_data(subject_ids, shhs_path)\n",
    "\n",
    "        all_base_ds = list()\n",
    "        for raw_fname, ann_fname in zip(self.raw_files, self.edf_files):\n",
    "            raw, desc = self._load_raw(\n",
    "                raw_fname,\n",
    "                ann_fname,\n",
    "                preload=preload,\n",
    "                crop_wake_mins=crop_wake_mins,\n",
    "                crop=crop\n",
    "            )\n",
    "            base_ds = BaseDataset(raw, desc)\n",
    "            all_base_ds.append(base_ds)\n",
    "        super().__init__(all_base_ds)\n",
    "    \n",
    "    def _fetch_data(\n",
    "        self,\n",
    "        subject_ids,\n",
    "        shhs_path,\n",
    "    ):\n",
    "        shhs_files = os.listdir(shhs_path)\n",
    "        for subject in subject_ids:\n",
    "            current_file = f\"SN{subject:03d}.edf\"\n",
    "            if  current_file in shhs_files:\n",
    "                self.raw_files.append(os.path.join(shhs_path, current_file))\n",
    "                self.edf_files.append(os.path.join(shhs_path, f\"SN{subject:03d}_sleepscoring.edf\"))        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_raw(\n",
    "        raw_fname,\n",
    "        ann_fname,\n",
    "        preload,\n",
    "        crop_wake_mins,\n",
    "        crop,\n",
    "    ):\n",
    "        raw = mne.io.read_raw_edf(raw_fname, preload=preload)\n",
    "        annots = mne.read_annotations(ann_fname)\n",
    "        raw.set_annotations(annots, emit_warning=False)\n",
    "        raw.resample(sfreq, npad=\"auto\")\n",
    "\n",
    "        if crop_wake_mins > 0:\n",
    "            # Find first and last sleep stages\n",
    "            mask = [x[-1] in [\"1\", \"2\", \"3\", \"R\"] for x in annots.description]\n",
    "            sleep_event_inds = np.where(mask)[0]\n",
    "\n",
    "            # Crop raw\n",
    "            tmin = annots[int(sleep_event_inds[0])][\"onset\"] - crop_wake_mins * 60\n",
    "            tmax = annots[int(sleep_event_inds[-1])][\"onset\"] + crop_wake_mins * 60\n",
    "            raw.crop(tmin=max(tmin, raw.times[0]), tmax=min(tmax, raw.times[-1]))\n",
    "\n",
    "        if crop is not None:\n",
    "            raw.crop(*crop)\n",
    "\n",
    "        raw_basename = os.path.basename(raw_fname)\n",
    "        subj_nb = int(raw_basename[2:5])\n",
    "        desc = pd.Series({\"subject_id\": subj_nb,}, name=\"\")\n",
    "        return raw, desc\n",
    "\n",
    "    \n",
    "def __get_epochs(windows_subject):\n",
    "    epochs_data = []\n",
    "    for epoch in windows_subject.windows:\n",
    "        epochs_data.append(epoch)\n",
    "    epochs_data = np.stack(epochs_data, axis=0) # Shape of (num_epochs, num_channels, num_sample_points)\n",
    "    return epochs_data\n",
    "\n",
    "\n",
    "shhs_dataset = SHHSSleepStaging(shhs_path=SHHS_PATH)\n",
    "shhs_windows_dataset = create_windows_from_events(\n",
    "                        shhs_dataset,\n",
    "                        window_size_samples=window_size_samples,\n",
    "                        window_stride_samples=window_size_samples,\n",
    "                        preload=False,\n",
    "                        mapping=mapping,\n",
    "                    )\n",
    "preprocess(shhs_windows_dataset, [Preprocessor(zscore)])\n",
    "\n",
    "SHHS_SAVE_PATH = os.path.join(os.path.split(SHHS_PATH)[0], 'subjects_data')\n",
    "if not os.path.exists(SHHS_SAVE_PATH):\n",
    "    os.makedirs(SHHS_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "for windows_subject in tqdm(shhs_windows_dataset.datasets, desc=\"SHHS dataset preprocessing ...\"):\n",
    "    shhs_subject_data = __get_epochs(windows_subject)\n",
    "\n",
    "    subjects_save_path = os.path.join(SHHS_SAVE_PATH, f\"{windows_subject.description['subject_id']:03d}.npz\")\n",
    "    np.savez(subjects_save_path, \n",
    "             eeg=shhs_subject_data[:, :4], \n",
    "             emg=shhs_subject_data[:, 4:5],\n",
    "             eog=shhs_subject_data[:, 5:7],\n",
    "             emog=shhs_subject_data[:, 4:7],\n",
    "             ecg=shhs_subject_data[:, 7:],\n",
    "             y=windows_subject.y,\n",
    "             subject_id=windows_subject.description['subject_id'],\n",
    "             epoch_length=len(windows_subject),\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SaO2',\n",
       " 'H.R.',\n",
       " 'EEG(sec)',\n",
       " 'ECG',\n",
       " 'EMG',\n",
       " 'EOG(L)',\n",
       " 'EOG(R)',\n",
       " 'EEG',\n",
       " 'SOUND',\n",
       " 'AIRFLOW',\n",
       " 'THOR RES',\n",
       " 'ABDO RES',\n",
       " 'POSITION',\n",
       " 'LIGHT',\n",
       " 'NEW AIR',\n",
       " 'OX stat']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.ch_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5b3f32a9b2f4e2cb14fef27893c5c45989f94303a36e8219b910dbd820a4653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
